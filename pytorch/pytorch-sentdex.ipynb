{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Introduction\n",
    " \n",
    "---\n",
    "\n",
    "Running numpy on GPU, with some nice helper functions.\n",
    "\n",
    "GPU responsible for small calculations, have a lot more cores than CPU\n",
    "\n",
    "CPU responsible for large calcultions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 5.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.Tensor([3,5])\n",
    "y = torch.Tensor([2,1])\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.zeros([3,1])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9406, 0.7457, 0.2727, 0.8933, 0.9393],\n",
       "        [0.4339, 0.1879, 0.1300, 0.6488, 0.8180]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.rand([2,5])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape: flatten the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9406, 0.7457, 0.2727, 0.8933, 0.9393, 0.4339, 0.1879, 0.1300, 0.6488,\n",
       "         0.8180]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = y.view([1,10])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Data\n",
    "---\n",
    "\n",
    "- Is the data machine-learnable?\n",
    "- 90% of time is spent on data collection\n",
    "- Vision is the main interest\n",
    "\n",
    "- Training and testing data sets\n",
    "\n",
    "> hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MNIST: hand-drawn digits from 0 to 9\n",
    "- 28 x 28 image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST(\"\", train=False, download=True, \n",
    "                      transform = transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Use batch_size, instead of passing data in one go\n",
    "- because data cannot fit into GPU\n",
    "- and to hope that the data will generalise\n",
    "- batch_size between 8 and 64\n",
    "---\n",
    "- shuffle: to make neural network generalise/ learn general rules instead of quicker routes\n",
    "\n",
    "16:00\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Iterate over data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([1, 8, 0, 2, 5, 7, 2, 3, 4, 9])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainset:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "# x is the image\n",
    "# y is the label\n",
    "\n",
    "x, y = data[0][0], data[1][0]\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1230a79d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reshape the tensor to 28 by 28\n",
    "\n",
    "plt.imshow(data[0][3].view(28,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balancing dataset\n",
    "---\n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "# printing how balanced the dataset is\n",
    "\n",
    "total = 0\n",
    "counter_dict = {0:0,1:0,2:0,3:0,4:0,5:0,6:0,7:0,8:0,9:0 }\n",
    "\n",
    "for data in trainset:\n",
    "    Xs, ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total +=1\n",
    "        \n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "# distribution of dataset\n",
    "\n",
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3. Building neural network\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn # OOP\n",
    "import torch.nn.functional as F # Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # define the fully connected layers to the network\n",
    "        # 784 = 28 x 28: flattened\n",
    "        # three layers of 64 neurons\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "       \n",
    "    \n",
    "    # how data is flow through the network\n",
    "    # feed-forward network\n",
    "    # activation function, adjust weight, biases\n",
    "    def forward(self, x):\n",
    "        # rectified linear: activation function: whether the neuron is firing, \n",
    "        # stepper/sigmoid function, loss explosion\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        # \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "        \n",
    "        \n",
    "     \n",
    "net = Net()\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1,28*28)\n",
    "\n",
    "# -1 specify that the input is unknown shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.2046, -2.2746, -2.2510, -2.3563, -2.4195, -2.3982, -2.3750, -2.2165,\n",
       "         -2.2122, -2.3486]], grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = net(X)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4. Pass through labeled data and train the model\n",
    "---\n",
    "- loss\n",
    "- measure of how wrong the model is\n",
    "- we want to decrease loss\n",
    "---\n",
    "\n",
    "- optimizer\n",
    "- adjust weights based on loss, gradients to lower the loss\n",
    "- calculate loss based on the output, comapre to desired output\n",
    "- we optimize for loss, not for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1093, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0792, grad_fn=<NllLossBackward>)\n",
      "tensor(0.0468, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# learning rate: cannot be too large or small\n",
    "# decaying learning rate\n",
    "optimizer = optim.Adam(net.parameters(), lr = 0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainset:\n",
    "        # data is a batch of feature sets and Labels\n",
    "        X, y = data\n",
    "        \n",
    "        # zero gradients before start\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # pass data through the network\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        \n",
    "        # how wrong were we\n",
    "        loss = F.nll_loss(output, y)\n",
    "        \n",
    "        # back propagate the loss\n",
    "        loss.backward()\n",
    "        \n",
    "        # adjust weights\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.981\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# we don't want to calculate the gradient\n",
    "# we just want to know how good the network is\n",
    "# so use no_grad()\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 784))\n",
    "        \n",
    "        # for every prediction, does it match the actual target value\n",
    "        for idx, i in enumerate(output):\n",
    "            if torch.argmax(i)== y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "            \n",
    "\n",
    "print(\"Accuracy: \", round(correct/total,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
       "\n",
       "\n",
       "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "          [0., 0., 0.,  ..., 0., 0., 0.]]]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAN2UlEQVR4nO3dbYxc5XnG8evCGAMGR14MrgGDDZjyUhUTbUxpaAsiICBSAKlFcavIVWk2H2IKKB9KKRVUSiXUNCCippacYMV5KRFpErCKW3AsJCtt5LJQxzY4YKCm2DE2xBU2LzFe++6HPaAFZp5dZs68rO//TxrNzLnnzLl11pfPmXlm5nFECMDh74heNwCgOwg7kARhB5Ig7EAShB1I4shubuwoT4ujNb2bmwRS+bXe1Dux341qbYXd9lWS7pM0RdI3I+Lu0uOP1nRd5Mvb2SSAgvWxtmmt5dN421MkfV3S1ZLOk7TY9nmtPh+AzmrnNfsiSc9HxIsR8Y6k70u6tp62ANStnbCfIunlMfe3V8vex/aQ7WHbwwe0v43NAWhHx9+Nj4jlETEYEYNTNa3TmwPQRDth3yFp7pj7p1bLAPShdsL+hKQFtufbPkrSZyWtqqctAHVreegtIkZsL5X0qEaH3lZExNO1dQagVm2Ns0fEakmra+oFQAfxcVkgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSaGsWV3THEdOnF+txzrymtWc/X173r/7gX4v1oY/9sli/Zedgsb5q0wVNa6f9oHysmf7Uy8X6yM5XinW8X1tht71N0j5JByWNRET5Lw+gZ+o4sl8WEa/V8DwAOojX7EAS7YY9JD1m+0nbQ40eYHvI9rDt4QPa3+bmALSq3dP4SyJih+2TJK2x/YuIWDf2ARGxXNJySZrhgWhzewBa1NaRPSJ2VNe7Jf1Y0qI6mgJQv5bDbnu67ePfvS3pSkmb62oMQL0c0dqZte0zNHo0l0ZfDvxzRPxdaZ0ZHoiLfHlL25vM9l/9iWJ95OZfFesLT9hRrN978n9+5J4mgy+/9tvF+vD1C4r1kRe31djN5LA+1mpv7HGjWsuv2SPiRUnNPzEBoK8w9AYkQdiBJAg7kARhB5Ig7EASfMW1C06+4/lifeW8n3Spk8nljlkbi/Vzbvq9Yv2sW7fV2M3kx5EdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0L/nvNucX6W3++ulg/1kfV2c5h4xOLnivW/69LfUwWHNmBJAg7kARhB5Ig7EAShB1IgrADSRB2IAnG2bvgtLvKP/V84bylxfqzVyyvsx0kxZEdSIKwA0kQdiAJwg4kQdiBJAg7kARhB5JgnL0PzHmk/Gd461PvFOul77t//L6biuvGOP/dn3X1C8X6D84qfxe/Hf878naxvvtvzyjWp2pPne1MeuMe2W2vsL3b9uYxywZsr7G9tbqe2dk2AbRrIqfx35J01QeW3SZpbUQskLS2ug+gj40b9ohYJ33ofOhaSSur2yslXVdvWwDq1upr9tkRsbO6/Yqk2c0eaHtI0pAkHa1jW9wcgHa1/W58RISkKNSXR8RgRAxO1bR2NwegRa2GfZftOZJUXe+uryUAndBq2FdJWlLdXiLp4XraAdApHj0LLzzAfkDSpZJmSdol6U5JD0l6UNJpkl6SdENEjDuoOcMDcZEvb6/jhJ5btqhc/8yyprXXD/26uO7qN08v1hcfv6tY76QLfrakWJ/7h5uL9YzWx1rtjT1uVBv3DbqIWNykRGqBSYSPywJJEHYgCcIOJEHYgSQIO5AEX3GdBM6991fF+uNXHN20dtkx5efu5dDag2+cVKzPu/NAsX6wzmYS4MgOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kwzt4HppwwUKw/+zczivXLjil/jbWXHnnrY01r3/2jK4rrHnr6F3W3kxpHdiAJwg4kQdiBJAg7kARhB5Ig7EAShB1IgnH2Ljhyzm8U6yc/tK9Yf/jUb9bZTledOGVv05pHDnWxE3BkB5Ig7EAShB1IgrADSRB2IAnCDiRB2IEkGGfvgl3XzC/WHzr1H7vUSfctmtZ8SvADJ04vrsuRqF7j7k/bK2zvtr15zLK7bO+wvaG6XNPZNgG0ayL/eX5L0lUNlt8bEQury+p62wJQt3HDHhHrJO3pQi8AOqidl0VLbW+sTvNnNnuQ7SHbw7aHD2h/G5sD0I5Ww75M0pmSFkraKemrzR4YEcsjYjAiBqdqWoubA9CulsIeEbsi4mBEHJL0DUmL6m0LQN1aCrvtOWPuXi9pc7PHAugP446z235A0qWSZtneLulOSZfaXigpJG2T9IXOtTj5zdr4RrH+s/1TivWLpx2eM5Gff8+mYv25q04s1g+++mqd7Rz2xg17RCxusPj+DvQCoIP4kBKQBGEHkiDsQBKEHUiCsANJOKL5VxDrNsMDcZEv79r2JotdN/1usf61W/+pWC8Nze2PA8V1b93R3t/j0wM/L9ePfb3l5z7738sjumffONzycx+u1sda7Y09blTjyA4kQdiBJAg7kARhB5Ig7EAShB1IgrADSTDOPglMOXdBsX7gpOOa1jxS/vv6Pza00tJ74uILivVH/mVFy8/92Nvln5r+2lnntPzchyvG2QEQdiALwg4kQdiBJAg7kARhB5Ig7EASTNk8CRzcsrVYP2JLlxppYOovmQZwsuDIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM6OoiPnn16sv76sPN00+se4R3bbc20/bvsZ20/bvrlaPmB7je2t1fXMzrcLoFUTOY0fkfSliDhP0u9I+qLt8yTdJmltRCyQtLa6D6BPjRv2iNgZEU9Vt/dJ2iLpFEnXSlpZPWylpOs61COAGnyk1+y250m6UNJ6SbMjYmdVekXS7CbrDEkakqSjdWzLjQJoz4Tfjbd9nKQfSrolIvaOrcXor1Y2/GXDiFgeEYMRMThV09pqFkDrJhR221M1GvTvRcSPqsW7bM+p6nMk7e5MiwDqMO5pvG1Lul/Sloi4Z0xplaQlku6urh/uSIfQa0MXF+t/8hePtvzc33lhUbH+5fMfKtavPObNlrc9nmXbLxvnETvHqWOsibxm/6Skz0naZHtDtex2jYb8Qds3SnpJ0g0d6RBALcYNe0T8VFLDH52XxIwPwCTBx2WBJAg7kARhB5Ig7EAShB1Igq+4TgJvfuqNYv2mmeWfmi6uO9j6up32P4/OL9ZPZZz9I+HIDiRB2IEkCDuQBGEHkiDsQBKEHUiCsANJMM4+CRzYMb3XLXTE+ev+rFg/4yv/Vaw3/GkkNMWRHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSYJx9Ejj7jk3F+nn7lzatrfvjrxTXnTXlmJZ6etfZ//aFYv03v/5209qZz5a/S39oZKSlntAYR3YgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSMIR5W8F254r6duSZmv0K8TLI+I+23dJ+rykV6uH3h4Rq0vPNcMDcZGZ+BXolPWxVntjT8NZlyfyoZoRSV+KiKdsHy/pSdtrqtq9EfEPdTUKoHMmMj/7Tml06o2I2Gd7i6RTOt0YgHp9pNfstudJulDS+mrRUtsbba+wPbPJOkO2h20PH9D+9roF0LIJh932cZJ+KOmWiNgraZmkMyUt1OiR/6uN1ouI5RExGBGDUzWt/Y4BtGRCYbc9VaNB/15E/EiSImJXRByMiEOSviFpUefaBNCuccNu25Lul7QlIu4Zs3zOmIddL2lz/e0BqMtE3o3/pKTPSdpke0O17HZJi20v1Ohw3DZJ5e86Auipibwb/1NJjcbtimPqAPoLn6ADkiDsQBKEHUiCsANJEHYgCcIOJEHYgSQIO5AEYQeSIOxAEoQdSIKwA0kQdiAJwg4kMe5PSde6MftVSS+NWTRL0mtda+Cj6dfe+rUvid5aVWdvp0fEiY0KXQ37hzZuD0fEYM8aKOjX3vq1L4neWtWt3jiNB5Ig7EASvQ778h5vv6Rfe+vXviR6a1VXeuvpa3YA3dPrIzuALiHsQBI9Cbvtq2w/a/t527f1oodmbG+zvcn2BtvDPe5lhe3dtjePWTZge43trdV1wzn2etTbXbZ3VPtug+1retTbXNuP237G9tO2b66W93TfFfrqyn7r+mt221MkPSfpCknbJT0haXFEPNPVRpqwvU3SYET0/AMYtn9f0huSvh0Rv1Ut+3tJeyLi7uo/ypkR8Zd90ttdkt7o9TTe1WxFc8ZOMy7pOkl/qh7uu0JfN6gL+60XR/ZFkp6PiBcj4h1J35d0bQ/66HsRsU7Sng8svlbSyur2So3+Y+m6Jr31hYjYGRFPVbf3SXp3mvGe7rtCX13Ri7CfIunlMfe3q7/mew9Jj9l+0vZQr5tpYHZE7KxuvyJpdi+baWDcaby76QPTjPfNvmtl+vN28Qbdh10SER+XdLWkL1anq30pRl+D9dPY6YSm8e6WBtOMv6eX+67V6c/b1Yuw75A0d8z9U6tlfSEidlTXuyX9WP03FfWud2fQra5397if9/TTNN6NphlXH+y7Xk5/3ouwPyFpge35to+S9FlJq3rQx4fYnl69cSLb0yVdqf6binqVpCXV7SWSHu5hL+/TL9N4N5tmXD3edz2f/jwiun6RdI1G35F/QdJf96KHJn2dIenn1eXpXvcm6QGNntYd0Oh7GzdKOkHSWklbJf1E0kAf9fYdSZskbdRosOb0qLdLNHqKvlHShupyTa/3XaGvruw3Pi4LJMEbdEAShB1IgrADSRB2IAnCDiRB2IEkCDuQxP8DBIwm428I7/QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[5].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[5].view(-1,784))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
